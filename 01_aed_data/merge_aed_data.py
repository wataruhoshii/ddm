"""
å·å´å¸‚AEDãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
4ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆï¼š
1. å·å´å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¬å…±æ–½è¨­ï¼‰
2. å…¨å›½AEDãƒãƒƒãƒ—ï¼ˆqqzaidanmapï¼‰
3. ã‚»ãƒ–ãƒ³-ã‚¤ãƒ¬ãƒ–ãƒ³è¨­ç½®åº—èˆ—ãƒªã‚¹ãƒˆ
4. aedm.jpï¼ˆå…¨å›½AEDãƒãƒƒãƒ— - ä¸€èˆ¬æŠ•ç¨¿å«ã‚€ï¼‰
"""

import pandas as pd
import json
import re
from typing import Dict, List, Tuple
import time
import requests

def load_kawasaki_opendata() -> pd.DataFrame:
    """å·å´å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“„ å·å´å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv('kawasaki_aed_utf8.csv')
    df = df.dropna(axis=1, how='all')
    
    # çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
    result = pd.DataFrame({
        'id': df['å°å¸³ç•ªå·'].apply(lambda x: f"kawasaki_{x}"),
        'source': 'å·å´å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿',
        'name': df['è¨­ç½®å ´æ‰€'],
        'address': df['ä½æ‰€'],
        'address_detail': df['è¨­ç½®ä½ç½®'],
        'facility_type': 'å…¬å…±æ–½è¨­',
        'available_24h': df['24æ™‚é–“365æ—¥åˆ©ç”¨å¯èƒ½ã‹'].apply(lambda x: '365æ—¥24æ™‚é–“ä½¿ç”¨å¯' in str(x)),
        'available_time': df.apply(lambda r: f"{r['åˆ©ç”¨é–‹å§‹æ™‚é–“']} - {r['åˆ©ç”¨çµ‚äº†æ™‚é–“']}" if pd.notna(r['åˆ©ç”¨é–‹å§‹æ™‚é–“']) else '', axis=1),
        'latitude': df['ç·¯åº¦'],
        'longitude': df['çµŒåº¦'],
        'everyone_allow': df['ä½¿ç”¨å¯¾è±¡è€…ã®ç¯„å›²'].apply(lambda x: 'å¤–éƒ¨' in str(x)),
        'note': df['ä½¿ç”¨å¯èƒ½æ—¥ãƒ»ä½¿ç”¨å¯èƒ½æ™‚é–“å¸¯ã®è£œè¶³']
    })
    
    print(f"  â†’ {len(result)}ä»¶")
    return result

def load_national_map() -> pd.DataFrame:
    """å…¨å›½AEDãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“„ å…¨å›½AEDãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv('kawasaki_aed_national_map.csv')
    
    result = pd.DataFrame({
        'id': df['id'].apply(lambda x: f"national_{x}"),
        'source': 'å…¨å›½AEDãƒãƒƒãƒ—',
        'name': df['install_location_name'],
        'address': df['install_address'].apply(lambda x: str(x).replace('å·å´å¸‚', '') if pd.notna(x) else ''),
        'address_detail': df['install_address_detail'],
        'facility_type': df['install_type_name'],
        'available_24h': df['use_everyday'],
        'available_time': df['available_time'],
        'latitude': df['latitude'],
        'longitude': df['longitude'],
        'everyone_allow': df['everyone_allow'].apply(lambda x: x == 'èªã‚ã‚‹'),
        'note': df['note']
    })
    
    print(f"  â†’ {len(result)}ä»¶")
    return result

def geocode_address(address: str) -> Tuple[float, float]:
    """ä½æ‰€ã‹ã‚‰ç·¯åº¦çµŒåº¦ã‚’å–å¾—ï¼ˆå›½åœŸåœ°ç†é™¢APIä½¿ç”¨ï¼‰"""
    try:
        # ä½æ‰€ã‚’æ­£è¦åŒ–
        full_address = f"ç¥å¥ˆå·çœŒå·å´å¸‚{address}" if not address.startswith('ç¥å¥ˆå·') else address
        
        url = "https://msearch.gsi.go.jp/address-search/AddressSearch"
        params = {'q': full_address}
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if data and len(data) > 0:
            coords = data[0].get('geometry', {}).get('coordinates', [])
            if len(coords) == 2:
                return coords[1], coords[0]  # lat, lng
    except Exception as e:
        pass
    
    return None, None

def load_seven_eleven() -> pd.DataFrame:
    """ã‚»ãƒ–ãƒ³-ã‚¤ãƒ¬ãƒ–ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä»˜ãï¼‰"""
    print("ğŸ“„ ã‚»ãƒ–ãƒ³-ã‚¤ãƒ¬ãƒ–ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_excel('kawasaki_711_aed.xlsx')
    
    print("  ğŸŒ ä½æ‰€ã‹ã‚‰ç·¯åº¦çµŒåº¦ã‚’å–å¾—ä¸­ï¼ˆå°‘ã€…ãŠå¾…ã¡ãã ã•ã„ï¼‰...")
    
    latitudes = []
    longitudes = []
    
    for i, row in df.iterrows():
        address = row['ä½æ‰€']
        lat, lng = geocode_address(address)
        latitudes.append(lat)
        longitudes.append(lng)
        
        if (i + 1) % 20 == 0:
            print(f"    é€²æ—: {i + 1}/{len(df)}")
        
        time.sleep(0.2)  # APIè² è·è»½æ¸›
    
    result = pd.DataFrame({
        'id': df['No'].apply(lambda x: f"seven_{x}"),
        'source': 'ã‚»ãƒ–ãƒ³-ã‚¤ãƒ¬ãƒ–ãƒ³ï¼ˆå·å´å¸‚å”å®šï¼‰',
        'name': df['åº—å'],
        'address': df['ä½æ‰€'],
        'address_detail': 'åº—èˆ—å†…',
        'facility_type': 'å•†æ¥­æ–½è¨­ï¼ˆã‚³ãƒ³ãƒ“ãƒ‹ï¼‰',
        'available_24h': True,  # 24æ™‚é–“å–¶æ¥­
        'available_time': '24æ™‚é–“',
        'latitude': latitudes,
        'longitude': longitudes,
        'everyone_allow': True,
        'note': 'å·å´å¸‚ãƒ»ã‚»ãƒ–ãƒ³-ã‚¤ãƒ¬ãƒ–ãƒ³å”å®šï¼ˆ2025å¹´10æœˆã€œï¼‰'
    })
    
    # åº§æ¨™å–å¾—æˆåŠŸç‡
    success = result['latitude'].notna().sum()
    print(f"  â†’ {len(result)}ä»¶ï¼ˆåº§æ¨™å–å¾—æˆåŠŸ: {success}ä»¶ï¼‰")
    
    return result

def load_aedm() -> pd.DataFrame:
    """aedm.jpï¼ˆå…¨å›½AEDãƒãƒƒãƒ—ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“„ aedm.jp ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv('kawasaki_aed_aedm_v2.csv')
    
    result = pd.DataFrame({
        'id': df['id'].apply(lambda x: f"aedm_{x}"),
        'source': 'aedm.jp',
        'name': df['name'],
        'address': df['address'].apply(lambda x: str(x).replace('å·å´å¸‚', '').replace('ç¥å¥ˆå·çœŒå·å´å¸‚', '') if pd.notna(x) else ''),
        'address_detail': '',
        'facility_type': '',
        'available_24h': False,
        'available_time': df['able'],
        'latitude': df['lat'],
        'longitude': df['lng'],
        'everyone_allow': True,
        'note': df['source']  # aedmå†…ã®ã‚½ãƒ¼ã‚¹æƒ…å ±
    })
    
    print(f"  â†’ {len(result)}ä»¶")
    return result

def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """é‡è¤‡ã‚’é™¤å»ï¼ˆåº§æ¨™ãƒ™ãƒ¼ã‚¹ã§åˆ¤å®šï¼‰"""
    print("\nğŸ” é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    # é‡è¤‡ãƒãƒ¼ã‚­ãƒ³ã‚°ï¼ˆå„ªå…ˆé †ä½: å·å´å¸‚ > å…¨å›½AED(qqzaidan) > ã‚»ãƒ–ãƒ³ > aedm.jpï¼‰
    source_priority = {
        'å·å´å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿': 1,
        'å…¨å›½AEDãƒãƒƒãƒ—': 2,
        'ã‚»ãƒ–ãƒ³-ã‚¤ãƒ¬ãƒ–ãƒ³ï¼ˆå·å´å¸‚å”å®šï¼‰': 3,
        'aedm.jp': 4
    }
    df['_priority'] = df['source'].map(source_priority).fillna(5)
    
    # ã‚½ãƒ¼ãƒˆã—ã¦é‡è¤‡é™¤å»
    df = df.sort_values('_priority')
    
    # åº§æ¨™ã‚’å°æ•°ç‚¹5æ¡ã§ä¸¸ã‚ã¦é‡è¤‡åˆ¤å®š
    df['_lat_round'] = df['latitude'].round(5)
    df['_lng_round'] = df['longitude'].round(5)
    
    before = len(df)
    df = df.drop_duplicates(subset=['_lat_round', '_lng_round'], keep='first')
    
    # ä¸€æ™‚ã‚«ãƒ©ãƒ å‰Šé™¤
    df = df.drop(columns=['_priority', '_lat_round', '_lng_round'])
    
    removed = before - len(df)
    print(f"  â†’ é‡è¤‡é™¤å»: {removed}ä»¶")
    
    return df

def extract_ward(address: str) -> str:
    """ä½æ‰€ã‹ã‚‰åŒºã‚’æŠ½å‡º"""
    wards = ['å·å´åŒº', 'å¹¸åŒº', 'ä¸­åŸåŒº', 'é«˜æ´¥åŒº', 'å®®å‰åŒº', 'å¤šæ‘©åŒº', 'éº»ç”ŸåŒº']
    for ward in wards:
        if ward in str(address):
            return ward
    return 'ä¸æ˜'

def analyze_merged_data(df: pd.DataFrame):
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã®åˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿åˆ†æ")
    print("=" * 60)
    
    print(f"\nğŸ“ ç·ä»¶æ•°: {len(df)}ä»¶")
    
    # ã‚½ãƒ¼ã‚¹åˆ¥
    print("\nğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥:")
    for source, count in df['source'].value_counts().items():
        print(f"   {source}: {count}ä»¶")
    
    # åŒºåˆ¥
    df['ward'] = df['address'].apply(extract_ward)
    print("\nğŸ˜ï¸ åŒºåˆ¥:")
    for ward, count in df['ward'].value_counts().items():
        print(f"   {ward}: {count}ä»¶")
    
    # 24æ™‚é–“åˆ©ç”¨å¯èƒ½
    available_24h = df['available_24h'].sum()
    print(f"\nâ° 24æ™‚é–“åˆ©ç”¨å¯èƒ½: {available_24h}ä»¶ ({available_24h/len(df)*100:.1f}%)")
    
    # åº§æ¨™ã‚ã‚Š
    has_coords = df[['latitude', 'longitude']].notna().all(axis=1).sum()
    print(f"ğŸ—ºï¸ åº§æ¨™æƒ…å ±ã‚ã‚Š: {has_coords}ä»¶ ({has_coords/len(df)*100:.1f}%)")

def save_merged_data(df: pd.DataFrame):
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...")
    
    # CSV
    df.to_csv('kawasaki_aed_merged.csv', index=False, encoding='utf-8')
    print("  ğŸ“„ kawasaki_aed_merged.csv")
    
    # GeoJSON
    features = []
    for _, row in df.iterrows():
        if pd.notna(row['latitude']) and pd.notna(row['longitude']):
            feature = {
                "type": "Feature",
                "geometry": {
                    "type": "Point",
                    "coordinates": [row['longitude'], row['latitude']]
                },
                "properties": {
                    "id": row['id'],
                    "source": row['source'],
                    "name": row['name'],
                    "address": row['address'],
                    "address_detail": row['address_detail'],
                    "facility_type": row['facility_type'],
                    "available_24h": bool(row['available_24h']),
                    "available_time": row['available_time'],
                    "everyone_allow": bool(row['everyone_allow']),
                    "note": row['note'] if pd.notna(row['note']) else ''
                }
            }
            features.append(feature)
    
    geojson = {
        "type": "FeatureCollection",
        "features": features
    }
    
    with open('kawasaki_aed_merged.geojson', 'w', encoding='utf-8') as f:
        json.dump(geojson, f, ensure_ascii=False, indent=2)
    print("  ğŸ—ºï¸ kawasaki_aed_merged.geojson")

def main():
    print("\n" + "=" * 60)
    print("ğŸ¥ å·å´å¸‚AEDãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ„ãƒ¼ãƒ« ğŸ¥")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_kawasaki = load_kawasaki_opendata()
    df_national = load_national_map()
    df_seven = load_seven_eleven()
    df_aedm = load_aedm()
    
    # çµ±åˆ
    print("\nğŸ”— ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­...")
    df_merged = pd.concat([df_kawasaki, df_national, df_seven, df_aedm], ignore_index=True)
    print(f"  â†’ çµ±åˆå‰: {len(df_merged)}ä»¶")
    
    # é‡è¤‡é™¤å»
    df_merged = remove_duplicates(df_merged)
    print(f"  â†’ çµ±åˆå¾Œ: {len(df_merged)}ä»¶")
    
    # åˆ†æ
    analyze_merged_data(df_merged)
    
    # ä¿å­˜
    save_merged_data(df_merged)
    
    print("\n" + "=" * 60)
    print("âœ… çµ±åˆå®Œäº†!")
    print("=" * 60)

if __name__ == "__main__":
    main()

